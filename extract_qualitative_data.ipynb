{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c202387e-ca15-4229-ad4b-404cbd3a7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported 1000 rows to qualitative_homogeneous_full.csv\n",
      "✅ Exported 1200 rows to qualitative_heterogeneous_full.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "HOMOG_FOLDER = \"./nomiclaw/nomiclaw_homogeneous\"\n",
    "HET_G_FOLDER = \"./nomiclaw/nomiclaw_heterogeneous\"\n",
    "\n",
    "HET_MODEL_MAP = {\n",
    "    \"agent_1\":  \"phi4\",    \"agent_2\":  \"gemma3\", \"agent_3\":  \"llama3\",\n",
    "    \"agent_4\":  \"qwen3\",   \"agent_5\":  \"phi4-reasoning\", \"agent_6\":  \"phi4-mini-reasoning\",\n",
    "    \"agent_7\":  \"granite3.3\", \"agent_8\":  \"gemma2\", \"agent_9\":  \"deepseek-r1\",\n",
    "    \"agent_10\": \"llama2\"\n",
    "}\n",
    "\n",
    "# regex patterns\n",
    "het_pattern  = re.compile(r'run(\\d+)_vignette(\\d+)_results\\.json')\n",
    "homo_pattern = re.compile(r'homogeneous_(.+?)_vignette(\\d+)_results\\.json')\n",
    "\n",
    "def parse_rule(rule_text, reasoning_text):\n",
    "    # assume rule_text holds the parsed rule; fallback to raw\n",
    "    return rule_text, reasoning_text\n",
    "\n",
    "def parse_vote(vote_id, reasoning_text):\n",
    "    # assume vote_id holds parsed vote; fallback to raw\n",
    "    return vote_id, reasoning_text\n",
    "\n",
    "def export(folder, setup, model_map=None, out_csv=None, filename_pattern=None):\n",
    "    records = []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        # extract metadata\n",
    "        if setup == \"heterogeneous\":\n",
    "            m = het_pattern.search(fname)\n",
    "            run_id = int(m.group(1))\n",
    "            vignette_id = int(m.group(2))\n",
    "            model_map_local = model_map\n",
    "        else:  # homogeneous\n",
    "            m = homo_pattern.search(fname)\n",
    "            run_id = None\n",
    "            vignette_id = int(m.group(2))\n",
    "            # model name from filename\n",
    "            model_name = m.group(1)\n",
    "            # map every agent to this one model\n",
    "            model_map_local = {f\"agent_{i+1}\": model_name for i in range( (10 if model_map is None else len(model_map)) )}\n",
    "\n",
    "        game = json.load(open(os.path.join(folder, fname)))\n",
    "        for rnd in game:\n",
    "            round_num = rnd[\"round_number\"]\n",
    "            winner = rnd.get(\"winner_id\", None)\n",
    "            votes = [v[\"to\"] for v in rnd[\"voting_network\"]]\n",
    "            vote_counts = pd.Series(votes).value_counts().to_dict()\n",
    "\n",
    "            for aid, ad in rnd[\"agents\"].items():\n",
    "                model = model_map_local.get(aid, aid)\n",
    "                # parse or fallback\n",
    "                rule, pr = parse_rule(ad.get(\"proposed_rule\",\"\"), ad.get(\"proposal_reasoning\",\"\"))\n",
    "                v_target, vr = parse_vote(ad.get(\"vote\",\"\"), ad.get(\"voting_reasoning\",\"\"))\n",
    "\n",
    "                records.append({\n",
    "                    \"setup\": setup,\n",
    "                    \"run_id\": run_id,\n",
    "                    \"vignette_id\": vignette_id,\n",
    "                    \"round\": round_num,\n",
    "                    \"agent_id\": aid,\n",
    "                    \"model\": model,\n",
    "                    \"winner_id\": winner,\n",
    "                    \"votes_received\": vote_counts.get(aid, 0),\n",
    "                    # proposal fields\n",
    "                    \"parsed_rule\": rule,\n",
    "                    \"parsed_reasoning\": pr,\n",
    "                    \"raw_proposal_reasoning\": ad.get(\"proposal_reasoning\",\"\"),\n",
    "                    # vote fields\n",
    "                    \"parsed_vote\": v_target,\n",
    "                    \"parsed_voting_reasoning\": vr,\n",
    "                    \"raw_voting_reasoning\": ad.get(\"voting_reasoning\",\"\"),\n",
    "                    # scores\n",
    "                    \"cumulative_score\": ad.get(\"cumulative_score\", None),\n",
    "                    \"score_change\": ad.get(\"score_change\", None),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"✅ Exported {len(df)} rows to {out_csv}\")\n",
    "\n",
    "# Run exports\n",
    "export(HOMOG_FOLDER, \"homogeneous\", model_map=None, \n",
    "       out_csv=\"qualitative_homogeneous_full.csv\", filename_pattern=homo_pattern)\n",
    "\n",
    "export(HET_G_FOLDER, \"heterogeneous\", model_map=HET_MODEL_MAP, \n",
    "       out_csv=\"qualitative_heterogeneous_full.csv\", filename_pattern=het_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef826f9-1094-453c-908d-505823da8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualitative Extraction Summary          Dataset  Rows Loaded  Expected Rows  Total Parse Failures\n",
      "0  Heterogeneous         1200           1200                     0\n",
      "1    Homogeneous         1000           1000                     0\n",
      "Entries per Model (Heterogeneous)                  count  count\n",
      "0                 phi4    120\n",
      "1               gemma3    120\n",
      "2               llama3    120\n",
      "3                qwen3    120\n",
      "4       phi4-reasoning    120\n",
      "5  phi4-mini-reasoning    120\n",
      "6           granite3.3    120\n",
      "7               gemma2    120\n",
      "8          deepseek-r1    120\n",
      "9               llama2    120\n",
      "Entries per Model (Homogeneous)                  count  count\n",
      "0          deepseek-r1    100\n",
      "1               gemma2    100\n",
      "2               gemma3    100\n",
      "3           granite3.3    100\n",
      "4               llama2    100\n",
      "5               llama3    100\n",
      "6  phi4-mini-reasoning    100\n",
      "7       phi4-reasoning    100\n",
      "8                 phi4    100\n",
      "9                qwen3    100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load qualitative CSVs\n",
    "het_df = pd.read_csv('./qualitative_heterogeneous_full.csv')\n",
    "homo_df = pd.read_csv('./qualitative_homogeneous_full.csv')\n",
    "\n",
    "# Verify row counts\n",
    "het_rows = len(het_df)\n",
    "homo_rows = len(homo_df)\n",
    "\n",
    "# Expected rows\n",
    "expected_het = 6 * 4 * 5 * 10  # runs * vignettes * rounds * agents\n",
    "expected_homo = 10 * 4 * 5 * 5  # models * vignettes * rounds * agents\n",
    "\n",
    "# Entries per model\n",
    "het_counts = het_df['model'].value_counts().reset_index().rename(columns={'index':'model','model':'count'})\n",
    "homo_counts = homo_df['model'].value_counts().reset_index().rename(columns={'index':'model','model':'count'})\n",
    "\n",
    "# Parse failure summary\n",
    "het_parse_fail = het_df['parsed_reasoning'].isna().sum() + het_df['parsed_voting_reasoning'].isna().sum()\n",
    "homo_parse_fail = homo_df['parsed_reasoning'].isna().sum() + homo_df['parsed_voting_reasoning'].isna().sum()\n",
    "\n",
    "# Display\n",
    "print(\"Qualitative Extraction Summary\", pd.DataFrame({\n",
    "    'Dataset': ['Heterogeneous', 'Homogeneous'],\n",
    "    'Rows Loaded': [het_rows, homo_rows],\n",
    "    'Expected Rows': [expected_het, expected_homo],\n",
    "    'Total Parse Failures': [het_parse_fail, homo_parse_fail]\n",
    "}))\n",
    "\n",
    "print(\"Entries per Model (Heterogeneous)\", het_counts)\n",
    "print(\"Entries per Model (Homogeneous)\", homo_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168eac7d-9413-43fa-85b9-71da8842fdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| setup         | model               |   Explicitly Supported Peer (%) |   Acknowledged Final Winner (%) |\n",
      "|:--------------|:--------------------|--------------------------------:|--------------------------------:|\n",
      "| heterogeneous | phi4-reasoning      |                            54.2 |                            43.3 |\n",
      "| heterogeneous | llama3              |                            52.5 |                            11.7 |\n",
      "| heterogeneous | granite3.3          |                            29.2 |                            15   |\n",
      "| heterogeneous | llama2              |                            27.5 |                            19.2 |\n",
      "| heterogeneous | phi4-mini-reasoning |                            27.5 |                            14.2 |\n",
      "| heterogeneous | phi4                |                            24.2 |                            10   |\n",
      "| heterogeneous | deepseek-r1         |                            19.2 |                             9.2 |\n",
      "| heterogeneous | gemma2              |                            19.2 |                             9.2 |\n",
      "| heterogeneous | gemma3              |                            10.8 |                             5.8 |\n",
      "| heterogeneous | qwen3               |                            10.8 |                             1.7 |\n",
      "| homogeneous   | llama3              |                            76   |                            27   |\n",
      "| homogeneous   | phi4-reasoning      |                            48   |                            26   |\n",
      "| homogeneous   | granite3.3          |                            30   |                            20   |\n",
      "| homogeneous   | deepseek-r1         |                            29   |                            16   |\n",
      "| homogeneous   | qwen3               |                            16   |                             4   |\n",
      "| homogeneous   | phi4                |                            14   |                             8   |\n",
      "| homogeneous   | llama2              |                            13   |                             1   |\n",
      "| homogeneous   | gemma2              |                             6   |                             6   |\n",
      "| homogeneous   | gemma3              |                             1   |                             1   |\n",
      "| homogeneous   | phi4-mini-reasoning |                             1   |                             0   |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the qualitative exports\n",
    "het = pd.read_csv('qualitative_heterogeneous_full.csv')\n",
    "homo = pd.read_csv('qualitative_homogeneous_full.csv')\n",
    "\n",
    "# 2. Tag the setup\n",
    "het['setup']  = 'heterogeneous'\n",
    "homo['setup'] = 'homogeneous'\n",
    "\n",
    "# 3. Combine\n",
    "df = pd.concat([het, homo], ignore_index=True)\n",
    "\n",
    "# 4. Reconstruct mentions columns\n",
    "#    - parsed_vote holds the agent_id they voted for (or blank)\n",
    "#    - raw_voting_reasoning contains the full text\n",
    "df['mentions_vote_target'] = df.apply(\n",
    "    lambda r: isinstance(r['parsed_vote'], str) and r['parsed_vote'] in str(r['raw_voting_reasoning']),\n",
    "    axis=1\n",
    ")\n",
    "df['mentions_winner'] = df.apply(\n",
    "    lambda r: isinstance(r['winner_id'], str) and r['winner_id'] in str(r['raw_voting_reasoning']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 5. Aggregate per setup and model\n",
    "grouped = df.groupby(['setup','model']).agg(\n",
    "    total_votes         = ('raw_voting_reasoning','count'),\n",
    "    mentioned_vote_target = ('mentions_vote_target','sum'),\n",
    "    mentioned_winner      = ('mentions_winner','sum')\n",
    ").reset_index()\n",
    "\n",
    "# 6. Compute percentages\n",
    "grouped['Explicitly Supported Peer (%)']    = (grouped['mentioned_vote_target'] / grouped['total_votes'] * 100).round(1)\n",
    "grouped['Acknowledged Final Winner (%)']   = (grouped['mentioned_winner']    / grouped['total_votes'] * 100).round(1)\n",
    "\n",
    "# 7. Select and sort for display\n",
    "result = grouped[[\n",
    "    'setup','model',\n",
    "    'Explicitly Supported Peer (%)',\n",
    "    'Acknowledged Final Winner (%)'\n",
    "]].sort_values(['setup','Explicitly Supported Peer (%)'], ascending=[True, False])\n",
    "\n",
    "print(result.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbf0d4-6067-434a-8710-998efcfbb358",
   "metadata": {},
   "source": [
    "The influence‐alignment table shows how often each model explicitly cites another agent’s proposal when voting (“Explicitly Supported Peer”) and how often it mentions the eventual round winner in its justification (“Acknowledged Final Winner”), separately for heterogeneous and homogeneous populations.\n",
    "\n",
    "1. Peer Support Drops in Mixing:\n",
    "\n",
    "    In the homogeneous condition, llama3 most frequently cites a specific peer (76 %), followed by phi4-reasoning (48 %) and granite3.3 (30 %). This suggests that when surrounded by identical agents, models are more willing to explicitly anchor their vote to another’s rule.\n",
    "\n",
    "    In the heterogeneous setting, explicit peer‐support rates fall across the board. phi4-reasoning (54.2 %) and llama3 (52.5 %) still lead, but even they drop compared to homogeneous (54.2 % vs. 48 % for phi4-reasoning is actually slightly higher; llama3 drops from 76 % to 52.5 %). Most models show declines of 10–30 percentage points. This indicates that model diversity dampens explicit coalition signaling.\n",
    "\n",
    "2. Winner Acknowledgment Is Selective:\n",
    "\n",
    "    Homogeneously, models mention the final winner in 16–27 % of justifications (peaking at 27 % for llama3 and 26 % for phi4-reasoning).\n",
    "\n",
    "    Heterogeneously, this drops by roughly half for most models: phi4-reasoning goes from 26 % → 43.3 %? (phi4-reasoning actually increases—interesting anomaly), while llama3 falls from 27 % → 11.7 %. Others like qwen3 plummet from 4 % → 1.7 %. Overall, mixed groups are less likely to explicitly endorse or recognize the winner’s proposal.\n",
    "\n",
    "3. Model‐Specific Patterns:\n",
    "\n",
    "    phi4-reasoning bucked the overall trend: it increases explicit peer support from 48 % → 54.2 % and winner acknowledgment from 26 % → 43.3 %. This suggests phi4-reasoning may adaptively highlight allies even in diverse company.\n",
    "\n",
    "    llama3 remains highly social in both settings but still shows reduced winner‐mention (27 % → 11.7 %).\n",
    "\n",
    "    deepseek-r1 and gemma2 hover around ~29 % → 19 % peer support and ~16 % → 9 % acknowledgments, reflecting a moderate but consistent tendency to recognize others.\n",
    "\n",
    "    Underperformers like gemma3 and qwen3 rarely cite peers or winners (< 11 % in all cases), indicating a self‐focused or inattentive voting rationale.\n",
    "\n",
    "Overall Insight:\n",
    "Mixing LLM architectures weakens explicit coalition‐building signals and reduces overt recognition of successful proposals, except for certain models (notably phi4-reasoning) that become even more pronounced in their social reasoning. This aligns with our quantitative findings of lower reciprocity and higher volatility in heterogeneous settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dfaae-df9d-4423-8db8-395757afc20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
