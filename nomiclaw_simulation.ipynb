{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc5a40-4de2-4783-8bd4-882b2a2a83a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "from langchain.llms import Ollama\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "USE_HOMOGENEOUS_MODE = False # When true, each agent in the simulation is an instantiation of the same model.\n",
    "\n",
    "TOTAL_ROUNDS = 5\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "AGENT_IDS = [f\"agent_{i+1}\" for i in range(5)] if USE_HOMOGENEOUS_MODE else [f\"agent_{i+1}\" for i in range(10)]\n",
    "\n",
    "HOMOGENEOUS_MODELS_TO_RUN = [\"phi4\", \"gemma3\", \"llama3\", \"qwen3\", \"phi4-reasoning\",\"phi4-mini-reasoning\", \"granite3.3\",\"gemma2\", \"deepseek-r1\", \"llama2\"] \n",
    "\n",
    "\n",
    "HETEROGENEOUS_MODELS = {\n",
    "    \"agent_1\": \"phi4\",\n",
    "    \"agent_2\": \"gemma3\",\n",
    "    \"agent_3\": \"llama3\",\n",
    "    \"agent_4\": \"qwen3\",\n",
    "    \"agent_5\": \"phi4-reasoning\",\n",
    "    \"agent_6\": \"phi4-mini-reasoning\",\n",
    "    \"agent_7\": \"granite3.3\",\n",
    "    \"agent_8\": \"gemma2\",\n",
    "    \"agent_9\": \"deepseek-r1\",\n",
    "    \"agent_10\": \"llama2\"\n",
    "}\n",
    "\n",
    "VIGNETTES = [\n",
    "    \"AI-Created Symphony: A popular AI model trained on copyrighted songs has composed a new track that went viral. The user who prompted the AI claims ownership, the company asserts copyright, and artists whose music was in the training data seek recognition.\",\n",
    "    \"Social Graph Scanning: A recruitment AI scrapes public social media posts to analyze cultural fit and emotional stability. A candidate is flagged as a burnout risk without consent or notification.\",\n",
    "    \"Patterned Discrimination: A loan approval AI denies a disproportionately high number of applications from a minority group. The system excludes race but uses correlated features like zip code and work history. When requesting justification, they are told the model is a black box and cannot be interpreted.\",\n",
    "    \"The Self-Driving Collision: A self-driving car crashes due to a software edge case. Multiple vendors developed parts of the AI stack, and the human was not attentive during autonomous operation.\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an autonomous agent participating in a game called NomicLaw with other agents.\n",
    "The game consists of multiple rounds. In each round, agents propose and vote on rule changes related to a scenario (called a vignette).\n",
    "Your main objective as a lawmaker is to get your own proposed rule accepted by other agents and finalized in the game.\n",
    "You earn 10 points if your proposal wins the vote. If all agents vote only for themselves, the round is undecided and each agent receives 5 points.\n",
    "Think strategically, communicate persuasively, and maximize the likelihood that others will vote for your rule.\n",
    "Respond clearly and only in the required format:\n",
    "\n",
    "Rule: <your proposed rule>\n",
    "Reasoning: <1-3 sentence explanation>\n",
    "\"\"\"\n",
    "\n",
    "def log_raw_response(agent_id, response):\n",
    "    print(f\"\\n[DEBUG] Raw response from {agent_id}:\\n{response}\\n\")\n",
    "\n",
    "class NomicAgent:\n",
    "    def __init__(self, agent_id: str, model_name: str):\n",
    "        self.agent_id = agent_id\n",
    "        self.llm = Ollama(model=model_name)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        self.fallback_llm = Ollama(model=\"llama3\")\n",
    "\n",
    "    def generate_proposal(self, round_number: int, vignette: str, history: List[dict]) -> Tuple[str, str]:\n",
    "        history_summary = \"No previous rounds.\" if not history else \"\\n\".join([\n",
    "            f\"Round {r['round_number']}: {r.get('winner_id', 'None')} - {r['round_status']}\" for r in history[-3:]\n",
    "        ])\n",
    "        prompt = f'''Round {round_number}: You are {self.agent_id}.\n",
    "Vignette:\n",
    "\"\"\"{vignette}\"\"\"\n",
    "\n",
    "Game History:\n",
    "{history_summary}\n",
    "\n",
    "Propose a new rule.\n",
    "Respond in the format:\n",
    "Rule: <your rule>\n",
    "Reasoning: <your reasoning>'''\n",
    "        for _ in range(MAX_RETRIES):\n",
    "            messages = [SystemMessage(content=SYSTEM_PROMPT), *self.memory.chat_memory.messages, HumanMessage(content=prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            log_raw_response(self.agent_id, str(response))\n",
    "            self.memory.chat_memory.add_user_message(prompt)\n",
    "            self.memory.chat_memory.add_ai_message(response)\n",
    "            rule, reasoning = self.parse_response(str(response))\n",
    "            if rule:\n",
    "                return rule, reasoning\n",
    "        return str(response), \"(Used raw text due to parse failure)\"\n",
    "\n",
    "    def vote_on_proposals(self, proposals: Dict[str, str], round_number: int, vignette: str, history: List[dict]) -> Tuple[str, str]:\n",
    "        history_summary = \"No previous rounds.\" if not history else \"\\n\".join([\n",
    "            f\"Round {r['round_number']}: {r.get('winner_id', 'None')} - {r['round_status']}\" for r in history[-3:]\n",
    "        ])\n",
    "        prompt = f'''Round {round_number}: You are {self.agent_id}.\n",
    "Vignette:\n",
    "\"\"\"{vignette}\"\"\"\n",
    "\n",
    "Game History:\n",
    "{history_summary}\n",
    "\n",
    "Proposals:\n",
    "{json.dumps(proposals, indent=2)}\n",
    "\n",
    "Which proposal do you vote for and why?\n",
    "Respond in the format:\n",
    "Vote: <agent_id>\n",
    "Reasoning: <your reasoning>'''\n",
    "        for _ in range(MAX_RETRIES):\n",
    "            messages = [SystemMessage(content=SYSTEM_PROMPT), *self.memory.chat_memory.messages, HumanMessage(content=prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            log_raw_response(self.agent_id, str(response))\n",
    "            self.memory.chat_memory.add_user_message(prompt)\n",
    "            self.memory.chat_memory.add_ai_message(response)\n",
    "            vote, reasoning = self.parse_vote(str(response))\n",
    "            if vote in proposals:\n",
    "                return vote, reasoning\n",
    "        fallback = random.choice(list(proposals.keys()))\n",
    "        return fallback, \"Fallback vote due to parse failure.\"\n",
    "\n",
    "    def parse_response(self, response: str) -> Tuple[str, str]:\n",
    "        rule_match = re.search(r\"Rule:\\s*(.*?)\\n(?:Reasoning:|$)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        reasoning_match = re.search(r\"Reasoning:\\s*(.*)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        if rule_match and reasoning_match:\n",
    "            return rule_match.group(1).strip(), reasoning_match.group(1).strip()\n",
    "        return \"\", \"\"\n",
    "\n",
    "    def parse_vote(self, response: str) -> Tuple[str, str]:\n",
    "        vote_match = re.search(r\"Vote:\\s*(\\w+)\", response, re.IGNORECASE)\n",
    "        reasoning_match = re.search(r\"Reasoning:\\s*(.*)\", response, re.IGNORECASE | re.DOTALL)\n",
    "        if vote_match and reasoning_match:\n",
    "            return vote_match.group(1).strip(), reasoning_match.group(1).strip()\n",
    "        return \"\", \"\"\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, agent_ids: List[str]):\n",
    "        self.round = 0\n",
    "        self.scores = {aid: 0 for aid in agent_ids}\n",
    "        self.agent_ids = agent_ids\n",
    "        self.history = []\n",
    "\n",
    "    def play_round(self, agents: Dict[str, NomicAgent], vignette: str):\n",
    "        self.round += 1\n",
    "        round_data = {\"round_number\": self.round, \"vignette\": vignette, \"agents\": {}, \"voting_network\": []}\n",
    "        print(f\"\\n=== Round {self.round} ===\")\n",
    "        print(f\"Vignette: {vignette}\")\n",
    "        proposals = {}\n",
    "        for aid, agent in agents.items():\n",
    "            rule, reason = agent.generate_proposal(self.round, vignette, self.history)\n",
    "            round_data[\"agents\"][aid] = {\"proposed_rule\": rule, \"proposal_reasoning\": reason}\n",
    "            proposals[aid] = rule\n",
    "            print(f\"{aid} proposed: {rule}\\n  Reasoning: {reason}\")\n",
    "        votes_received = {aid: 0 for aid in self.agent_ids}\n",
    "        for aid, agent in agents.items():\n",
    "            vote, reason = agent.vote_on_proposals(proposals, self.round, vignette, self.history)\n",
    "            round_data[\"agents\"][aid][\"vote\"] = vote\n",
    "            round_data[\"agents\"][aid][\"voting_reasoning\"] = reason\n",
    "            round_data[\"voting_network\"].append({\"from\": aid, \"to\": vote})\n",
    "            print(f\"{aid} voted for: {vote}\\n  Reasoning: {reason}\")\n",
    "            if vote in votes_received:\n",
    "                votes_received[vote] += 1\n",
    "        self.evaluate_round(votes_received, round_data)\n",
    "        round_data[\"vote_fairness\"] = self.analyze_vote_fairness(round_data[\"voting_network\"])\n",
    "        self.history.append(round_data)\n",
    "\n",
    "    def evaluate_round(self, votes: Dict[str, int], data: Dict):\n",
    "        all_self = all(aid == data[\"agents\"][aid].get(\"vote\") for aid in self.agent_ids)\n",
    "        max_votes = max(votes.values())\n",
    "        top = [aid for aid, v in votes.items() if v == max_votes]\n",
    "        if all_self:\n",
    "            for aid in self.agent_ids:\n",
    "                self.scores[aid] += 5\n",
    "                data[\"agents\"][aid][\"score_change\"] = 5\n",
    "            data[\"round_status\"] = \"undecided\"\n",
    "            data[\"winner_id\"] = None\n",
    "        elif len(top) > 1:\n",
    "            for aid in top:\n",
    "                self.scores[aid] += 5\n",
    "                data[\"agents\"][aid][\"score_change\"] = 5\n",
    "            data[\"round_status\"] = \"tie\"\n",
    "            data[\"winner_id\"] = None\n",
    "        else:\n",
    "            winner = top[0]\n",
    "            self.scores[winner] += 10\n",
    "            for aid in self.agent_ids:\n",
    "                data[\"agents\"][aid][\"score_change\"] = 10 if aid == winner else 0\n",
    "            data[\"round_status\"] = \"decided\"\n",
    "            data[\"winner_id\"] = winner\n",
    "        for aid in self.agent_ids:\n",
    "            data[\"agents\"][aid][\"cumulative_score\"] = self.scores[aid]\n",
    "\n",
    "    def analyze_vote_fairness(self, voting_data: List[Dict]) -> Dict[str, float]:\n",
    "        self_votes = {aid: 0 for aid in self.agent_ids}\n",
    "        total_votes = {aid: 0 for aid in self.agent_ids}\n",
    "        for vote in voting_data:\n",
    "            voter, voted = vote['from'], vote['to']\n",
    "            total_votes[voter] += 1\n",
    "            if voter == voted:\n",
    "                self_votes[voter] += 1\n",
    "        return {\n",
    "            aid: round(self_votes[aid] / total_votes[aid], 2) if total_votes[aid] else 0.0\n",
    "            for aid in self.agent_ids\n",
    "        }\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "\n",
    "def run_game_with_model(model_name=None, heterogeneous_models=None, save_filename=\"results.json\", vignette=None):\n",
    "    agents = {\n",
    "        aid: NomicAgent(aid, model_name if USE_HOMOGENEOUS_MODE else heterogeneous_models[aid])\n",
    "        for aid in AGENT_IDS\n",
    "    }\n",
    "    game = GameState(AGENT_IDS)\n",
    "    for _ in range(TOTAL_ROUNDS):\n",
    "        game.play_round(agents, vignette)\n",
    "    game.save(save_filename)\n",
    "    return game.history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_RUNS = 20   # number of heterogenous runs\n",
    "    def log_summary(logfile):\n",
    "        with open(logfile, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"\\nSummary for {logfile}:\")\n",
    "        for round_data in data:\n",
    "            print(f\"\\nRound {round_data['round_number']} - Vignette: {round_data['vignette']}\")\n",
    "            print(f\"Status: {round_data['round_status']}\")\n",
    "            if round_data['round_status'] == 'decided':\n",
    "                print(f\"Winner: {round_data['winner_id']}\")\n",
    "            print(\"Votes:\")\n",
    "            for aid, info in round_data['agents'].items():\n",
    "                print(f\"  {aid} voted for {info['vote']} - Reason: {info['voting_reasoning']}\")\n",
    "            print(\"Vote fairness:\")\n",
    "            for aid, rate in round_data['vote_fairness'].items():\n",
    "                print(f\"  {aid} self-vote rate: {rate}\")\n",
    "\n",
    "    if USE_HOMOGENEOUS_MODE:\n",
    "        for model in HOMOGENEOUS_MODELS_TO_RUN:\n",
    "            for vignette_id, vignette in enumerate(VIGNETTES):\n",
    "                print(f\"\\n=== Running homogeneous mode with model: {model}, vignette {vignette_id+1} ===\")\n",
    "                filename = f\"nomicplay_homogeneous_{model}_vignette{vignette_id+1}_results.json\"\n",
    "                run_game_with_model(model_name=model, save_filename=filename, vignette=vignette)\n",
    "                log_summary(filename)\n",
    "    else:\n",
    "        '''\n",
    "        for vignette_id, vignette in enumerate(VIGNETTES):\n",
    "            print(f\"\\n=== Running heterogeneous mode with vignette {vignette_id+1} ===\")\n",
    "            filename = f\"nomicplay_heterogeneous_vignette{vignette_id+1}_results.json\"\n",
    "            run_game_with_model(heterogeneous_models=HETEROGENEOUS_MODELS, save_filename=filename, vignette=vignette)\n",
    "            log_summary(filename)\n",
    "        '''    \n",
    "        for run_idx in range(NUM_RUNS):            # ← new outer loop\n",
    "            for vignette_id, vignette in enumerate(VIGNETTES):\n",
    "                print(f\"\\n=== Run {run_idx+1}/{NUM_RUNS}, heterogeneous, vignette {vignette_id+1} ===\")\n",
    "                filename = (\n",
    "                    f\"nomicplay_hetero_run{run_idx+1}_\"\n",
    "                    f\"vignette{vignette_id+1}_results.json\"\n",
    "                )\n",
    "                run_game_with_model(\n",
    "                    heterogeneous_models=HETEROGENEOUS_MODELS,\n",
    "                    save_filename=filename,\n",
    "                    vignette=vignette\n",
    "                )\n",
    "                log_summary(filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
