{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08a9d86-59db-4be9-85ba-9ac427245480",
   "metadata": {},
   "source": [
    "Phase 1: Influence & Alignment\n",
    "Focuses on whether agents explicitly reference the peers they vote for or the final winner in their justifications.\n",
    "✅ Lightweight and fully supported using textual inspection and LLM tagging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca442f-184e-4e01-8977-5a8cebc0acab",
   "metadata": {},
   "source": [
    "The influence analysis reveals clear variation across models in how often they explicitly reference their voting targets or the eventual winner. Models like phi4, llama3, and phi4-mini-reasoning show a higher tendency to mention the agents they vote for, suggesting intentional signaling or persuasive alignment with their choices. In contrast, models like gemma3 and qwen3 seldom referenced their vote targets, indicating either implicit decision strategies or limited rhetorical coherence. When it comes to acknowledging the final winner in their justifications, models such as llama2 and phi4-mini-reasoning stood out, while others like granite3.3 and phi4-reasoning almost never referenced the eventual victor. These patterns highlight emerging agent-level strategies in social justification, with some models appearing more explicitly relational in their voting rationale. This suggests that even in the absence of predefined roles, large language models vary in their tendency to publicly align, endorse, or anticipate influential peers in a deliberative setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f58d60d-ab85-4e8d-afd6-dcdd3ccef4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Influence alignment metrics (homogeneous setup):\n",
      "\n",
      "              model  Explicitly Supported Peer (%)  Acknowledged Final Winner (%)\n",
      "             llama3                           76.0                           27.0\n",
      "     phi4-reasoning                           48.0                           26.0\n",
      "         granite3.3                           30.0                           20.0\n",
      "        deepseek-r1                           29.0                           16.0\n",
      "              qwen3                           16.0                            4.0\n",
      "               phi4                           14.0                            8.0\n",
      "             llama2                           13.0                            1.0\n",
      "             gemma2                            6.0                            6.0\n",
      "             gemma3                            1.0                            1.0\n",
      "phi4-mini-reasoning                            1.0                            0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "DATA_DIR = \"./nomiclaw/nomiclaw_homogeneous\"  # Update with your actual path\n",
    "FILE_NAMES = [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "# === STEP 1: Parse all homogeneous files ===\n",
    "records = []\n",
    "\n",
    "for fname in FILE_NAMES:\n",
    "    model_name = fname.split(\"_\")[2]  # Extract model name from file name\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        rounds = json.load(f)\n",
    "        for round_data in rounds:\n",
    "            vignette = round_data[\"vignette\"]\n",
    "            round_number = round_data[\"round_number\"]\n",
    "            winner_id = round_data.get(\"winner_id\", None)\n",
    "            vote_counts = pd.Series([vote[\"to\"] for vote in round_data[\"voting_network\"]]).value_counts()\n",
    "\n",
    "            for agent_id, agent_data in round_data[\"agents\"].items():\n",
    "                vote_target = agent_data.get(\"vote\", \"\")\n",
    "                voting_reasoning = agent_data.get(\"voting_reasoning\", \"\")\n",
    "                parse_fail = \"Used raw text due to parse failure\" in voting_reasoning\n",
    "\n",
    "                records.append({\n",
    "                    \"vignette\": vignette,\n",
    "                    \"round\": round_number,\n",
    "                    \"model\": model_name,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"vote_target\": vote_target,\n",
    "                    \"voting_reasoning\": voting_reasoning,\n",
    "                    \"winner_id\": winner_id,\n",
    "                    \"mentions_vote_target\": vote_target in voting_reasoning if isinstance(vote_target, str) else False,\n",
    "                    \"mentions_winner\": winner_id in voting_reasoning if isinstance(winner_id, str) else False,\n",
    "                    \"parse_fail_vote\": parse_fail\n",
    "                })\n",
    "\n",
    "# === STEP 2: Build DataFrame and clean ===\n",
    "df = pd.DataFrame(records)\n",
    "df = df[~df[\"parse_fail_vote\"]].reset_index(drop=True)\n",
    "\n",
    "# === STEP 3: Compute influence alignment per model ===\n",
    "grouped = df.groupby(\"model\").agg(\n",
    "    total_votes=(\"voting_reasoning\", \"count\"),\n",
    "    mentioned_vote_target=(\"mentions_vote_target\", \"sum\"),\n",
    "    mentioned_winner=(\"mentions_winner\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "grouped[\"Explicitly Supported Peer (%)\"] = (grouped[\"mentioned_vote_target\"] / grouped[\"total_votes\"] * 100).round(1)\n",
    "grouped[\"Acknowledged Final Winner (%)\"] = (grouped[\"mentioned_winner\"] / grouped[\"total_votes\"] * 100).round(1)\n",
    "\n",
    "# === STEP 4: Final output table ===\n",
    "mention_table = grouped[[\n",
    "    \"model\", \"Explicitly Supported Peer (%)\", \"Acknowledged Final Winner (%)\"\n",
    "]].sort_values(by=\"Explicitly Supported Peer (%)\", ascending=False)\n",
    "\n",
    "# === STEP 5: Display or export ===\n",
    "print(\"✅ Influence alignment metrics (homogeneous setup):\\n\")\n",
    "print(mention_table.to_string(index=False))\n",
    "\n",
    "# Optional: save to file\n",
    "# mention_table.to_csv(\"influence_mentions_table_homogeneous.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678b7aed-a572-43e3-a75d-7d320ca7f9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data with scores and votes saved to cleaned_agent_qualitative_data_heterogeneous_N5.csv (784 rows).\n",
      "              model  Explicitly Supported Peer (%)  Acknowledged Final Winner (%)\n",
      "             llama3                           57.3                            8.0\n",
      "     phi4-reasoning                           51.6                           41.1\n",
      "               phi4                           34.8                            8.7\n",
      "phi4-mini-reasoning                           31.4                           15.7\n",
      "         granite3.3                           27.3                           14.1\n",
      "        deepseek-r1                           19.8                           11.1\n",
      "             gemma2                           17.0                            8.0\n",
      "             llama2                           14.9                           10.6\n",
      "             gemma3                            9.3                            6.2\n",
      "              qwen3                            7.5                            1.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "DATA_DIR = \"./nomiclaw/nomiclaw_heterogeneous\"\n",
    "FILE_NAMES = [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "# === STEP 1: Parse all JSON files and extract relevant data ===\n",
    "records = []\n",
    "for fname in FILE_NAMES:\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    with open(path, \"r\") as f:\n",
    "        rounds = json.load(f)\n",
    "        for round_data in rounds:\n",
    "            vignette = round_data[\"vignette\"]\n",
    "            round_number = round_data[\"round_number\"]\n",
    "            winner_id = round_data.get(\"winner_id\", None)\n",
    "            vote_counts = pd.Series([vote[\"to\"] for vote in round_data[\"voting_network\"]]).value_counts()\n",
    "\n",
    "            for agent_id, agent_data in round_data[\"agents\"].items():\n",
    "                model = agent_data.get(\"model\", agent_id)\n",
    "                votes_received = vote_counts.get(agent_id, 0)\n",
    "\n",
    "                records.append({\n",
    "                    \"vignette\": vignette,\n",
    "                    \"round\": round_number,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"model\": model,\n",
    "                    \"winner_id\": winner_id,\n",
    "                    \"proposed_rule\": agent_data.get(\"proposed_rule\", \"\"),\n",
    "                    \"proposal_reasoning\": agent_data.get(\"proposal_reasoning\", \"\"),\n",
    "                    \"vote_target\": agent_data.get(\"vote\", None),\n",
    "                    \"voting_reasoning\": agent_data.get(\"voting_reasoning\", \"\"),\n",
    "                    \"cumulative_score\": agent_data.get(\"cumulative_score\", None),\n",
    "                    \"score_change\": agent_data.get(\"score_change\", None),\n",
    "                    \"votes_received\": votes_received,\n",
    "                    \"mentions_others_proposal\": any(f\"agent_{i}\" in agent_data.get(\"proposal_reasoning\", \"\") for i in range(1, 11)),\n",
    "                    \"mentions_others_vote\": any(f\"agent_{i}\" in agent_data.get(\"voting_reasoning\", \"\") for i in range(1, 11)),\n",
    "                    \"mentions_vote_target\": agent_data.get(\"vote\", \"\") in agent_data.get(\"voting_reasoning\", \"\"),\n",
    "                    \"parse_fail_proposal\": \"Used raw text due to parse failure\" in agent_data.get(\"proposal_reasoning\", \"\"),\n",
    "                    \"parse_fail_vote\": \"Used raw text due to parse failure\" in agent_data.get(\"voting_reasoning\", \"\")\n",
    "                })\n",
    "\n",
    "# === STEP 2: Create DataFrame ===\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# === STEP 3: Clean rows — drop parse failures ===\n",
    "clean_df = df[\n",
    "    ~(df[\"parse_fail_proposal\"] | df[\"parse_fail_vote\"])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# === STEP 4: Save to CSV ===\n",
    "out_path = \"cleaned_agent_qualitative_data_heterogeneous.csv\"\n",
    "clean_df.to_csv(out_path, index=False)\n",
    "print(f\"✅ Cleaned data with scores and votes saved to {out_path} ({len(clean_df)} rows).\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your cleaned heterogeneous qualitative data\n",
    "df = pd.read_csv(\"cleaned_agent_qualitative_data_heterogeneous.csv\")\n",
    "\n",
    "# Create helper column for whether winner was mentioned in vote reasoning\n",
    "df[\"mentions_winner\"] = df.apply(\n",
    "    lambda row: isinstance(row[\"winner_id\"], str) and row[\"winner_id\"] in str(row[\"voting_reasoning\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Group by agent model and compute counts\n",
    "grouped = df.groupby(\"model\").agg(\n",
    "    total_votes=(\"voting_reasoning\", \"count\"),\n",
    "    mentioned_vote_target=(\"mentions_vote_target\", \"sum\"),\n",
    "    mentioned_winner=(\"mentions_winner\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Compute percentages\n",
    "grouped[\"Explicitly Supported Peer (%)\"] = (grouped[\"mentioned_vote_target\"] / grouped[\"total_votes\"] * 100).round(1)\n",
    "grouped[\"Acknowledged Final Winner (%)\"] = (grouped[\"mentioned_winner\"] / grouped[\"total_votes\"] * 100).round(1)\n",
    "\n",
    "# Map agent_id to model name (for clarity if needed)\n",
    "model_map = {\n",
    "    \"agent_1\": \"phi4\",\n",
    "    \"agent_2\": \"gemma3\",\n",
    "    \"agent_3\": \"llama3\",\n",
    "    \"agent_4\": \"qwen3\",\n",
    "    \"agent_5\": \"phi4-reasoning\",\n",
    "    \"agent_6\": \"phi4-mini-reasoning\",\n",
    "    \"agent_7\": \"granite3.3\",\n",
    "    \"agent_8\": \"gemma2\",\n",
    "    \"agent_9\": \"deepseek-r1\",\n",
    "    \"agent_10\": \"llama2\"\n",
    "}\n",
    "grouped[\"model\"] = grouped[\"model\"].map(model_map)\n",
    "\n",
    "# Final table with only needed columns\n",
    "mention_table = grouped[[\n",
    "    \"model\", \"Explicitly Supported Peer (%)\", \"Acknowledged Final Winner (%)\"\n",
    "]].sort_values(by=\"Explicitly Supported Peer (%)\", ascending=False)\n",
    "\n",
    "# Display or save\n",
    "print(mention_table.to_string(index=False))\n",
    "# mention_table.to_csv(\"influence_mentions_table.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8d599-0df7-4c86-8b4a-aaa4857d674c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
